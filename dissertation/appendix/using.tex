
\section{Using \quim}\label{s:using}

Figure~\ref{fig:main} shows the main page of \quim\ User Interface, which follows the depicted structure: it is composed of a \textsf{Main Menu}, where the user can find the principal navigation options; a \textsf{Secondary Menu} that lists the options related to the actual page content; the \textsf{Sitemap}, which shows the user current location on the application and can be used to navigate through the website levels too; and the \textsf{Content} area, where the information is displayed.

\begin{figure}[h]
\begin{center} 
\includegraphics[scale=0.37]{img/qm-nav} 
\caption{\quim\ Main Page}\label{fig:main} 
\end{center} 
\end{figure} 

In what follows, it is briefly described how to use \quim\ and its major features.

\subsection{Contest and Problem Management}

%\quim\ user interface offers a complete back-end area to manage all the tasks related to a programming contest, as well as to the system functionalities.

The setting-up process for a new contest is performed by the system \textsf{Administrator}. 
Besides the contest name and duration, it associates to each contest a group of \textsf{Teachers} and a group of \textsf{Judges}. %, optionally, \textsf{Teams} or \textsf{Competitors} for compete in the contest.

After a contest creation, the associated \textsf{Teachers} can use \quim\ front-end to publish new problems, distributed by phases. 
Associated to each problem description, it is required to define the maximum execution time allowed and to provide a set of input values and the respective expected outputs for testing the solutions submitted by each competitor team.
When submitting a new test, \textsf{Teachers} can define if it will be publicly available as example for competitors, or if it will be hidden and only used for grading purposes.

In their turn, associated \textsf{Judges} can consult the current submissions and the evaluation statistics. They can also follow the contest flow by accessing the contest statistics, and it will be asked to them to assign a final grade to each submission.

To participate in a contest, \textsf{Competitors} need to register in the system and
to be associated to the intended contest.
This association can be done directly by the \textsf{Administrator}, by the \textsf{Teachers} or
by the \textsf{Competitor} itself.
For this purpose and after choosing  the desired contest in the \emph{Active Contest} list (which can be accessed through the Secondary Menu of \quim\ Main Page depicted in Figure~\ref{fig:main}),
the user should select the option "Join Contest" on the Secondary Menu of the Contest Preview page
(as depicted in Figure~\ref{fig:contest}).

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{img/qm-contest}
\caption{\quim\ \textsf{Competitor} Interface --- Contest Main Page \label{fig:contest}}
\end{center}
\end{figure}


Moreover, the user must associate himself to a \textsf{Team}, as contests are organized in \textsf{Teams} and
not in individual competitors.
The same \textsf{Team} is allowed (and is encourage) to compete in several contests,
however in the same competition, a user can only be assigned to one \textsf{Team}.

\subsection{Answer Submission}

In a typical competitive environment, a \textsf{Team} can submit several solutions\footnote{In this context, the words  \emph{solution}, \emph{answer}, or \emph{submission} are alternative names to designate the program developed by the Competitor aimed at solving the problem (the programming challenge)
stated by the Teacher.} 
before one is accepted.
For final grading purposes, \quim\ considers the best ranked answer.
This allows the submission of several answers, developed by  different members of one \textsf{Team}, in order to get a better score.
This (not conventional) approach aims to stimulate the competition and to privilege the competitive learning,
allowing also to improve the code quality.

To submit a solution,
a \textsf{Competitor} starts  selecting the respective contest page.
It is then shown the problem statement and some examples of the desired input and the correspondent expected output. %, as depicted in Figure~\ref{fig:enunc}.
After selecting the option for submitting an answer, the user uploads the respective file.
If the upload process is unsuccessful, the system informs the user to try again.
If successful, \quim\ compiles the submitted (uploaded) program and notifies the user about this task.
In case of compilation errors, Quimera reports the errors found and provides some tips aiming at guiding the Competitor to a correct them and resubmit the answer. 
If no compilation errors are detected, the system goes to the next step, the solution assessment process.


\subsection{Solutions Assessment and Ranking}

Quimera ranks each submission based on the following levels: \emph{Didn't Compile}; \emph{Compile} (when it fails all tests); \emph{Timeout}; \emph{Passed some tests}; \emph{Solved} (as depicted in Figure~\ref{fig:problem}).

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.5]{img/qm-problem}
\caption{ \quim\ \textsf{Teacher} Interface --- List of submitted solutions}\label{fig:problem}
\end{center}
\end{figure}


As the execution of an external program is a critical task for any computer environment
(malicious  code can  damage the system),
\quim\ only compiles and executes the solution compiled if it complies with strong constraints
(consensual safety limits are defined).
With the help of the static analysis, system calls and suspicious instructions are blocked.
By default the execution timeout is 3 seconds, but this can be personalized at the problem creation moment.

After the dynamic assessment step, Quimera performs an analysis to the submitted solution analysis, based on a set of predefined metrics.
Each metric represents a measurement that contributes for the quality estimation, working like an indicator for \textsf{Teachers} and \textsf{Judges}.
Moreover, it allows to detect requirements infringements like not use in the solution a specific data structure. 

After the computation, metrics and other static assessment data are available through \quim\ interfaces, as detailed in~\ref{s:grading}.



\subsection{Assessment Report}\label{s:grading}

The assessment report produced by Quimera systems is composed of three different views, as can be seen in Figure~\ref{fig:answers}: 

\begin{itemize}

\item \emph{Default}, where is presented a grading summary concerning the grading formula described in Section~\ref{ss:formula};

\item \emph{Code}, where the user have a preview of the submitted source code;

\item \emph{Metrics}, where is presented the report about the submitted solution quality, based on the set of predefined metrics.

\end{itemize}

For a better comprehension of Quimera assessment report, it will be introduced one example of a contest where \textsf{Competitors} are invited to solve several mathematical problems. %, like the one proposed on Figure~\ref{fig:enunc}.
The challenge is to present a solution to calculate the \emph{n$^{th}$} fibonacci number, where \emph{n} is a number asked to the user.
Figure~\ref{fig:answers} shows the comparison between two different solutions proposed by two \textsf{Teams}: the \textsf{Rationals} on the left-hand size, and \textsf{Smarties} on the right-hand size.

\begin{figure}[h]
\centering
\includegraphics[scale=0.39]{img/qm-answers}
\caption{\quim\ Common Interface ---  Assessment report comparison}\label{fig:answers}
\end{figure}

The first solution, assessed by the system with "Solved Some Tests" (as can be seen in Figure~\ref{fig:problem}), only calculates correctly the first and second numbers of the sequence (0 and 1) and, consequently only passes in 22\% of the tests (2 in 9).
This leads to a final score on the \emph{Execution} field of only 22.2\%. 
In a closer look, it can be concluded that this answer is well documented (it has 15 commented lines in 50 lines of code) and its \emph{Legibility} has a final score of 93\%.
Its \emph{Complexity} score of 73\% is owed to its number of used variables and data structures.
Its 50 lines of code exceed the average size of the contest submissions for this problem, which penalizes its final grade on the \emph{Dimension} category (only 45.2\%).
In the \emph{Consistency} category, its 21.6\% are owed to the use of several returns through the code (assuming a maximum of two returns per function as a reasonable limit to the source code consistency) and also to the use of pointers.

The second one, assessed by the system with "Solved" (as depicted in Figure~\ref{fig:problem}), follows an iterative strategy to implement the Fibonacci recurrence. 
This answer is better documented (11 commented lines in 36 lines of code), but its \emph{Legibility} has a lower final score (88\%) once it did not used \emph{defines}, as the other solution.
Its \emph{Complexity} score of 58\% is owed to the implemented \emph{loop}.
Its 36 lines of code improve its final grade on the \emph{Dimension} category to 88.6\%.
In the \emph{Consistency} category, its weak 22\% are also owed to the use of several returns through the code.

Moreover, it can be concluded that both solutions have not been plagiarized (100\% original) and \emph{Competitors} follow the good practice of no repeating their own code (0\% of duplicated code).
These assessments led to a final grade of 29\% on the first case and a significant 89\% on the second case.
\quim\ also completes this evaluation with radial charts to a quicker and easier comparison of the solution performance in the different grading categories.
\\

Figure~\ref{fig:metrics} previews the complete quality report of one of the \textsf{Smarties} Team submissions, from the \textsf{Teacher}'s interface. 

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.48]{img/qm-metric}
\caption{\quim\ \textsf{Teacher} Interface ---  Assessment data for a submitted solution}\label{fig:metrics}
\end{center}
\end{figure}

\newpage
An XML full report is also generated.
The details behind the implementation of this analysis are introduced in Section~\ref{ss:tech}.

\section{Statistics}\label{s:stats}

As we seen during this Appendix, \quim\ computes several statistics to enable the monitoring of the contests, which depends on the active interface and context. Statistics can be concerned with different  perspectives
like:
all or a  particular contest;
all or a particular  problem;
all or a particular Team or Competitor (team member).
Statistics are presented in the form of different type of graphics: piecharts (see Figure~\ref{fig:main} and~\ref{fig:contest}), columns charts (see Figure~\ref{fig:group}), tables (see Figure~\ref{fig:problem}), line charts, radial charts (see Figure~\ref{fig:answers}) and scatter charts. 

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.46]{img/qm-group}
\caption{\quim\ \textsf{Teacher} Interface --- \textsf{Team} members grading}\label{fig:group}
\end{center}
\end{figure}

In more detail, it is possible to say that Quimera system offers numerical data and listings concerned with:
 \textsf{Competitors} and \textsf{Teams} rankings;
comparative graphics between problem submissions;
the full list of submissions for each \textsf{Competitor} or \textsf{Team};
comparisons between \textsf{Competitors} inside a \textsf{Team};
comparisons between the \textsf{Teams} performance in a context;
overviews for each contest, its phases and summary information about the current state of the problems and submissions,
among many others.

\newpage
This amount of statistical information makes \quim\ a powerful tool for supporting the assessment and grading of programming exercises solutions, 
helping all kind of users.%, \textsf{Administrator}, \textsf{Teachers},  \textsf{Judges} and \textsf{Competitors}.

